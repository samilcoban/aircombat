import numpy as np
import gymnasium as gym
from gymnasium import spaces
import math

from config import Config
from src.core import AirCombatCore
from aircombat_sim.utils.map_limits import MapLimits
from aircombat_sim.utils.geodesics import geodetic_distance_km, geodetic_bearing_deg


class AirCombatEnv(gym.Env):
    metadata = {"render_modes": ["rgb_array"]}

    def __init__(self):
        super().__init__()
        self.cfg = Config
        self.core = None

        # --- Maps ---
        self.map_limits = MapLimits(*self.cfg.MAP_LIMITS)

        # Tactical Zoom for Rendering
        center_lon = (self.cfg.MAP_LIMITS[0] + self.cfg.MAP_LIMITS[2]) / 2.0
        center_lat = (self.cfg.MAP_LIMITS[1] + self.cfg.MAP_LIMITS[3]) / 2.0
        zoom = 0.75
        self.render_limits = MapLimits(
            center_lon - zoom, center_lat - zoom,
            center_lon + zoom, center_lat + zoom
        )

        # --- Spaces ---
        # Actions: [Roll, G-Pull, Throttle, Fire, Countermeasures]
        self.action_space = spaces.Box(
            low=-1.0, high=1.0, shape=(self.cfg.ACTION_DIM,), dtype=np.float32
        )

        # Observations: Flattened Entity List
        self.observation_space = spaces.Box(
            low=-np.inf, high=np.inf, shape=(self.cfg.OBS_DIM,), dtype=np.float32
        )

        self.renderer = None
        self.blue_ids = []
        self.red_ids = []
        self.kappa = 0.0 # Default to perfect play

    def reset(self, seed=None, options=None):
        super().reset(seed=seed)
        rng = np.random.default_rng(seed)
        self.core = AirCombatCore()

        self.blue_ids = []
        self.red_ids = []

        center_lat = 0.5
        center_lon = 0.5

        # Spawn Blue Agents (South)
        for i in range(self.cfg.N_AGENTS):
            lat_pct = (center_lat - 0.02) + rng.uniform(-0.01, 0.01)  # Reduced from 0.05
            lon_pct = center_lon + ((i - self.cfg.N_AGENTS / 2) * 0.02)
            lat, lon = self.map_limits.absolute_position(lat_pct, lon_pct)
            # Spawn with heading 0 (North)
            # Increase speed to 900 km/h (approx 0.7 Mach) to prevent immediate stalls
            uid = self.core.spawn(lat, lon, 0, 900, "blue", "plane")
            self.blue_ids.append(uid)

        # Spawn Red Enemies (North)
        for i in range(self.cfg.N_ENEMIES):
            lat_pct = (center_lat + 0.02) + rng.uniform(-0.01, 0.01)  # Reduced from 0.05
            lon_pct = center_lon + ((i - self.cfg.N_ENEMIES / 2) * 0.02)
            lat, lon = self.map_limits.absolute_position(lat_pct, lon_pct)
            # Spawn with heading 180 (South)
            uid = self.core.spawn(lat, lon, 180, 900, "red", "plane")
            self.red_ids.append(uid)

        # Return observation for the first Blue agent
        info = {}
        if self.red_ids:
            info["red_obs"] = self._get_obs(self.red_ids[0])
        
        return self._get_obs(self.blue_ids[0]), info

    def set_kappa(self, k):
        """Called by training script to update curriculum difficulty"""
        self.kappa = k

    def _potential(self, x, x_mean, alpha):
        # L(w, x, xm)
        exponent = -alpha * (x - x_mean)
        exponent = np.clip(exponent, -20, 20) # Stability
        return (1.0 - np.exp(exponent)) / (1.0 + np.exp(exponent))

    def step(self, action, red_actions=None):
        # 1. Prepare Actions
        actions = {}
        agent_id = self.blue_ids[0]

        # Case A: Concatenated Action (Self-Play / Model)
        if len(action.shape) > 0 and action.shape[0] == 2 * self.cfg.ACTION_DIM:
            blue_action = action[:self.cfg.ACTION_DIM]
            red_action_in = action[self.cfg.ACTION_DIM:]
            if agent_id in self.core.entities: actions[agent_id] = blue_action
            if self.red_ids and self.red_ids[0] in self.core.entities:
                actions[self.red_ids[0]] = red_action_in
                
        # Case B: Single Action (Scripted Opponent)
        else:
            if agent_id in self.core.entities:
                actions[agent_id] = action
            # Red actions will be generated by core.step() automatically because they are missing from 'actions' dict
            
            # Inject Red Actions if provided explicitly (Legacy/Manual)
            if red_actions is not None:
                if self.red_ids and self.red_ids[0] in self.core.entities:
                    if isinstance(red_actions, (np.ndarray, list)):
                        actions[self.red_ids[0]] = red_actions
                    elif isinstance(red_actions, dict):
                        actions.update(red_actions)

        # --- SAFETY OVERRIDE (TRAINING WHEELS) ---
        override_active = False
        if agent_id in self.core.entities:
            agent = self.core.entities[agent_id]
            
            # "Hard Deck" at 2000m. If below, force pull up.
            if agent.alt < 2000.0:
                # If we are diving or banking hard near the ground
                if agent.pitch < 0.1 or abs(agent.roll) > 1.0:
                    override_active = True
                    
                    # Force Wings Level (Roll = 0)
                    # Force Max G (Pull = 1.0)
                    # Force Afterburner (Throttle = 1.0)
                    
                    # Proportional correction to level wings
                    roll_cmd = np.clip(-agent.roll * 2.0, -1.0, 1.0)
                    
                    # Construct override action: [Roll, G, Throttle, Fire, CM]
                    override_action = np.array([roll_cmd, 1.0, 1.0, 0.0, 0.0], dtype=np.float32)
                    
                    actions[agent_id] = override_action

        # 2. Step Physics Core (Pass Kappa)
        self.core.step(actions, self.kappa)

        # 3. Calculate Rewards
        reward = 0.0
        terminated = False
        truncated = False
        term_reason = "none"

        if agent_id not in self.core.entities:
            # Did we crash or get shot?
            death_event = next((e for e in self.core.events if e.get('victim') == agent_id), None)
            if death_event:
                if death_event['type'] == 'crash':
                    reward = -50.0 
                    term_reason = "crash"
                elif death_event['type'] == 'kill':
                    reward = -50.0 # Shot down
                    term_reason = "shot"
            else:
                reward = -50.0
                term_reason = "crash"
            
            terminated = True
        else:
            # 1. Existential Penalty (Time Pressure)
            # Increased slightly to prevent loitering
            reward -= 0.01 

            agent = self.core.entities[agent_id]

            # 2. Energy Reward (NEW - Intuition: Specific Excess Power)
            # Instead of just penalizing stall (<200), we reward maintaining high energy (Altitude + Speed).
            # This prevents the "Corner Velocity Trap" where agents fly at the minimum safe speed.
            # Normalized approx: Alt/20km + Speed/Mach2
            energy_score = (agent.alt / 20000.0) + (agent.speed / 1500.0)
            reward += energy_score * 0.01

            # 3. Shaping Reward (MODIFIED - Anti-Farming)
            # Previous coefficients (0.1) allowed gathering ~200 pts/episode just by following.
            # New coefficients (0.002) cap farming at ~5 pts, forcing the agent to seek the +100 Kill reward.
            nearest = None
            min_dist = float('inf')
            for e in self.core.entities.values():
                if e.team == "red":
                    d = geodetic_distance_km(agent.lat, agent.lon, e.lat, e.lon)
                    if d < min_dist:
                        min_dist = d
                        nearest = e

            if nearest:
                dist_m = min_dist * 1000.0
                bearing = geodetic_bearing_deg(agent.lat, agent.lon, nearest.lat, nearest.lon)
                
                # ATA (Angle to Target)
                ata = abs((bearing - agent.heading + 180) % 360 - 180)
                mu = np.radians(ata)
                
                # AA (Aspect Angle - Target's tail)
                bearing_to_me = (bearing + 180) % 360
                aa = abs((bearing_to_me - nearest.heading + 180) % 360 - 180)
                lam = np.radians(aa)

                # Aiming Reward (Reduced 50x)
                r_aim = 0.5 * (1.0 - (mu / np.pi)) ** 2
                reward += r_aim * 0.002

                # Geometry Reward (Reduced 50x)
                pos_potential = self._potential(lam/np.pi, 0.5, 18.0)
                r_geo = (1.0 - mu/np.pi) * pos_potential
                reward += r_geo * 0.002

                # Distance Reward (Reduced 50x)
                if ata < 60:
                    r_close = self._potential(dist_m, 900.0, 0.002) 
                    reward += r_close * 0.002

                # Lock Reward (The "WEZ" Bonus)
                # We reward the LOCK state more than just pointing, as this requires valid sensors
                if min_dist < self.cfg.MISSILE_RANGE_KM and ata < 45.0:
                    is_locking, _ = self.core.get_sensor_state(agent_id, nearest.uid)
                    if is_locking:
                        reward += 0.05  # Stronger signal: "You are ready to fire"

            # 4. Event Rewards (NEW - Shot Penalty)
            for ev in self.core.events:
                # Penalty for firing (Prevent Missile Spam)
                if ev['type'] == 'missile_fired' and ev['shooter'] == agent_id:
                    reward -= 2.0  # Cost of a missile. Forces high pKH (Probability of Kill) shots.

                if ev['type'] == 'kill':
                    if ev['killer'] in self.blue_ids: 
                        reward += 100.0  # The Jackpot
                    if ev['victim'] in self.blue_ids: 
                        reward -= 50.0

            # Win Condition
            reds_alive = sum(1 for e in self.core.entities.values() if e.team == "red")
            if reds_alive == 0:
                reward += 100.0
                terminated = True

        # 4. Check Time Limit
        if self.core.time >= self.cfg.MAX_DURATION_SEC:
            truncated = True
            term_reason = "timeout"

        info = {}
        info["termination_reason"] = term_reason
        if self.red_ids and self.red_ids[0] in self.core.entities:
            info["red_obs"] = self._get_obs(self.red_ids[0])
        else:
            # Dead Red Agent placeholder
            info["red_obs"] = np.zeros(self.cfg.OBS_DIM, dtype=np.float32)

        return self._get_obs(agent_id), reward, terminated, truncated, info

    def _get_obs(self, ego_id):
        vecs = []

        # 1. Ego Vector (Always present)
        if ego_id in self.core.entities:
            vecs.append(self._vectorize(self.core.entities[ego_id], ego_id, True))
        else:
            # Dead agent placeholder
            vecs.append(np.zeros(self.cfg.FEAT_DIM, dtype=np.float32))

        # 2. Other Entities (Friends, Enemies, Missiles)
        for uid, ent in self.core.entities.items():
            if uid == ego_id: continue

            # --- Sensor Logic (Fog of War) ---
            visible = True
            rwr_active = False

            if ego_id in self.core.entities and ent.team != "blue":
                # Can I see them? (Radar + Doppler)
                visible, _ = self.core.get_sensor_state(ego_id, uid)

                # Can they see me? (RWR)
                # Check if they are locking me
                _, locking_me = self.core.get_sensor_state(uid, ego_id)
                if locking_me:
                    rwr_active = True

            if visible:
                v = self._vectorize(ent, ego_id, False)
                # If visible + locking, set RWR flag in the visible vector
                if rwr_active: v[12] = 1.0
                vecs.append(v)
            elif rwr_active:
                # Not visible but Locking -> Ghost Vector (RWR only)
                v = np.zeros(self.cfg.FEAT_DIM, dtype=np.float32)
                v[12] = 1.0  # Set RWR flag
                vecs.append(v)
            else:
                # Hidden
                vecs.append(np.zeros(self.cfg.FEAT_DIM, dtype=np.float32))

        # 3. Padding
        flat = []
        for v in vecs: flat.extend(v)

        # Truncate if too many
        if len(flat) > self.cfg.OBS_DIM:
            flat = flat[:self.cfg.OBS_DIM]

        # Pad if too few
        if len(flat) < self.cfg.OBS_DIM:
            flat.extend([0.0] * (self.cfg.OBS_DIM - len(flat)))

        return np.array(flat, dtype=np.float32)

    def _vectorize(self, e, ego_id, is_ego):
        lat_n, lon_n = self.map_limits.relative_position(e.lat, e.lon)
        hr = math.radians(e.heading)

        # Sensor Signals
        rwr_signal = 0.0
        maws_signal = 0.0

        # MAWS: If it's a missile targeting me
        if e.type == "missile" and e.target_id == ego_id:
            maws_signal = 1.0

        return [
            lat_n, lon_n,
            np.cos(hr), np.sin(hr),
            e.speed / 1000.0,
            1.0 if e.team == "blue" else -1.0,
            1.0 if e.type == "missile" else 0.0,
            1.0 if is_ego else 0.0,
            np.cos(e.roll), np.sin(e.roll),
            np.cos(e.pitch), np.sin(e.pitch),
            rwr_signal,
            maws_signal,
            e.alt / 10000.0,
            # --- Logistics (Phase 5) ---
            e.fuel,
            e.ammo / float(self.cfg.MAX_MISSILES)
        ]

    def render(self):
        from aircombat_sim.utils.scenario_plotter import ScenarioPlotter, PlotConfig, Airplane, Missile
        import matplotlib.pyplot as plt

        if self.renderer is None:
            p_cfg = PlotConfig()
            p_cfg.units_scale = 20.0
            self.renderer = ScenarioPlotter(self.render_limits, dpi=100, config=p_cfg)

        drawables = []
        for e in self.core.entities.values():
            c = (0, 0, 1, 1) if e.team == "blue" else (1, 0, 0, 1)

            if e.type == "missile":
                drawables.append(Missile(e.lat, e.lon, e.heading, fill_color=c, zorder=10))
            else:
                # Detailed Info Text
                txt = f"{e.uid}\nA:{int(e.alt)}\nF:{int(e.fuel * 100)}%"
                if e.cm_active: txt += "\nCM!"

                drawables.append(Airplane(e.lat, e.lon, e.heading, fill_color=c, info_text=txt, zorder=5))

        try:
            fname = f"temp_render_{self.blue_ids[0] if self.blue_ids else 0}.png"
            self.renderer.to_png(fname, drawables)
            return (plt.imread(fname) * 255).astype(np.uint8)
        except:
            return np.zeros((400, 400, 3), dtype=np.uint8)